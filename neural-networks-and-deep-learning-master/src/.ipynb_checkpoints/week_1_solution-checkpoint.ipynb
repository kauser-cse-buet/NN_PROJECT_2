{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: ../data/mnist.pkl.gz Bytes: 17051982\n",
      "# Completed downloading file: mnist.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import urllib2\n",
    "# import urllib\n",
    "import io\n",
    "import gzip\n",
    "\n",
    "def download_file(remote, local):\n",
    "    url = remote\n",
    "\n",
    "    u = urllib2.urlopen(url)\n",
    "    f = open(local, 'wb')\n",
    "    meta = u.info()\n",
    "    \n",
    "    file_size = int(meta.getheaders(\"Content-Length\")[0])\n",
    "    print \"Downloading: %s Bytes: %s\" % (local, file_size)\n",
    "\n",
    "    file_size_dl = 0\n",
    "    block_sz = 8192\n",
    "    while True:\n",
    "        buffer = u.read(block_sz)\n",
    "        if not buffer:\n",
    "            break\n",
    "\n",
    "        file_size_dl += len(buffer)\n",
    "        f.write(buffer)\n",
    "        status = r\"%10d  [%3.2f%%]\" % (file_size_dl, file_size_dl * 100. / file_size)\n",
    "        status = status + chr(8)*(len(status)+1)\n",
    "#         print status,\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "data_dir = \"../data\"\n",
    "data_file_url = \"https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\"\n",
    "if os.path.exists(data_dir) == False:\n",
    "    os.makedirs(data_dir)\n",
    "    print(\"Directory created: \" + data_dir)\n",
    "\n",
    "    \n",
    "data_filename = \"mnist.pkl.gz\"\n",
    "data_filepath = data_dir + \"/\" + data_filename \n",
    "\n",
    "download_file(remote = data_file_url, local = data_filepath)\n",
    "\n",
    "\n",
    "# with urllib2.urlopen(data_file_url) as d:\n",
    "#     data = d.read()\n",
    "#     d.close()\n",
    "\n",
    "# with open(data_filepath, 'wb') as f:\n",
    "#     f.write(data)\n",
    "\n",
    "print(\"# Completed downloading file: \" + data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data, validation data and test data loaded.\n"
     ]
    }
   ],
   "source": [
    "import mnist_loader\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "print(\"Training data, validation data and test data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. \n",
      "# Size of each image: 784\n",
      "# Range of data: max = 0.99609375, min: 0.0\n",
      "# Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"3. \")\n",
    "\n",
    "image_train_list = []\n",
    "class_train_list = []\n",
    "\n",
    "for i in training_data:\n",
    "    image_train_list.append(i[0])\n",
    "    class_train_list.append(i[1])\n",
    "\n",
    "no_of_image_train, image_size_train, m = np.array(image_train_list).shape\n",
    "\n",
    "print(\"# Size of each image: \" + str(image_size_train))\n",
    "\n",
    "    \n",
    "max_image_data = np.max(image_train_list)\n",
    "min_image_data = np.min(image_train_list)\n",
    "\n",
    "print(\"# Range of data: max = \" + str(max_image_data) + \", min: \" + str(min_image_data))\n",
    "\n",
    "no_of_image_class, no_of_classes, class_dimension = np.array(class_train_list).shape\n",
    "\n",
    "print(\"# Number of classes: \"+ str(no_of_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# No of images in train: 50000\n",
      "# No of images in test: 10000\n",
      "# No of images in validation: 10000\n"
     ]
    }
   ],
   "source": [
    "image_test_list = []\n",
    "class_test_list = []\n",
    "\n",
    "image_validation_list = []\n",
    "class_validation_list = []\n",
    "\n",
    "for i in test_data:\n",
    "    image_test_list.append(i[0])\n",
    "    class_test_list.append(i[1])\n",
    "    \n",
    "for i in validation_data:\n",
    "    image_validation_list.append(i[0])\n",
    "    class_validation_list.append(i[1])\n",
    "\n",
    "no_of_image_test, image_size_test, m_test = np.array(image_test_list).shape\n",
    "no_of_image_validation, image_size_validation, m_validation = np.array(image_validation_list).shape\n",
    "\n",
    "\n",
    "print(\"# No of images in train: \" + str(no_of_image_train))\n",
    "print(\"# No of images in test: \" + str(no_of_image_test))\n",
    "print(\"# No of images in validation: \" + str(no_of_image_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<network.Network at 0x659ef98>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import network\n",
    "\n",
    "net = network.Network([784, 30, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9121 / 10000\n",
      "Epoch 1: 9225 / 10000\n",
      "Epoch 2: 9312 / 10000\n",
      "Epoch 3: 9369 / 10000\n",
      "Epoch 4: 9351 / 10000\n",
      "Epoch 5: 9380 / 10000\n",
      "Epoch 6: 9420 / 10000\n",
      "Epoch 7: 9425 / 10000\n",
      "Epoch 8: 9423 / 10000\n",
      "Epoch 9: 9471 / 10000\n",
      "Epoch 10: 9445 / 10000\n",
      "Epoch 11: 9446 / 10000\n",
      "Epoch 12: 9454 / 10000\n",
      "Epoch 13: 9492 / 10000\n",
      "Epoch 14: 9472 / 10000\n",
      "Epoch 15: 9484 / 10000\n",
      "Epoch 16: 9472 / 10000\n",
      "Epoch 17: 9444 / 10000\n",
      "Epoch 18: 9474 / 10000\n",
      "Epoch 19: 9482 / 10000\n",
      "Epoch 20: 9475 / 10000\n",
      "Epoch 21: 9477 / 10000\n",
      "Epoch 22: 9457 / 10000\n",
      "Epoch 23: 9473 / 10000\n",
      "Epoch 24: 9488 / 10000\n",
      "Epoch 25: 9482 / 10000\n",
      "Epoch 26: 9490 / 10000\n",
      "Epoch 27: 9472 / 10000\n",
      "Epoch 28: 9476 / 10000\n",
      "Epoch 29: 9478 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(epochs=30, mini_batch_size=10, training_data=training_data, test_data=test_data, eta=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Repeat training step 4,5 but for 100 hidden neurons\n",
      "Epoch 0: 5490 / 10000\n",
      "Epoch 1: 5638 / 10000\n",
      "Epoch 2: 5712 / 10000\n",
      "Epoch 3: 5658 / 10000\n",
      "Epoch 4: 5687 / 10000\n",
      "Epoch 5: 5731 / 10000\n",
      "Epoch 6: 5760 / 10000\n",
      "Epoch 7: 5731 / 10000\n",
      "Epoch 8: 5825 / 10000\n",
      "Epoch 9: 5893 / 10000\n",
      "Epoch 10: 6028 / 10000\n",
      "Epoch 11: 7498 / 10000\n",
      "Epoch 12: 7507 / 10000\n",
      "Epoch 13: 7503 / 10000\n",
      "Epoch 14: 7560 / 10000\n",
      "Epoch 15: 7534 / 10000\n",
      "Epoch 16: 7554 / 10000\n",
      "Epoch 17: 7580 / 10000\n",
      "Epoch 18: 7594 / 10000\n",
      "Epoch 19: 7590 / 10000\n",
      "Epoch 20: 7583 / 10000\n",
      "Epoch 21: 7572 / 10000\n",
      "Epoch 22: 7593 / 10000\n",
      "Epoch 23: 7601 / 10000\n",
      "Epoch 24: 7606 / 10000\n",
      "Epoch 25: 7601 / 10000\n",
      "Epoch 26: 7585 / 10000\n",
      "Epoch 27: 7588 / 10000\n",
      "Epoch 28: 7608 / 10000\n",
      "Epoch 29: 7599 / 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"6. Repeat training step 4,5 but for 100 hidden neurons\")\n",
    "no_hidden_neuron = 100\n",
    "net_100 = network.Network([784, no_hidden_neuron, 10])\n",
    "net_100.SGD(epochs=30, mini_batch_size=10, training_data=training_data, test_data=test_data, eta=3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. Repeat training step 4,5 but for a learning rate of 0.001 and another time for 100.0\n",
      "Epoch 0: 962 / 10000\n",
      "Epoch 1: 1012 / 10000\n",
      "Epoch 2: 984 / 10000\n",
      "Epoch 3: 982 / 10000\n",
      "Epoch 4: 982 / 10000\n",
      "Epoch 5: 982 / 10000\n",
      "Epoch 6: 982 / 10000\n",
      "Epoch 7: 983 / 10000\n",
      "Epoch 8: 994 / 10000\n",
      "Epoch 9: 1120 / 10000\n",
      "Epoch 10: 1263 / 10000\n",
      "Epoch 11: 1322 / 10000\n",
      "Epoch 12: 1344 / 10000\n",
      "Epoch 13: 1357 / 10000\n",
      "Epoch 14: 1396 / 10000\n",
      "Epoch 15: 1434 / 10000\n",
      "Epoch 16: 1481 / 10000\n",
      "Epoch 17: 1546 / 10000\n",
      "Epoch 18: 1605 / 10000\n",
      "Epoch 19: 1653 / 10000\n",
      "Epoch 20: 1698 / 10000\n",
      "Epoch 21: 1752 / 10000\n",
      "Epoch 22: 1813 / 10000\n",
      "Epoch 23: 1887 / 10000\n",
      "Epoch 24: 1925 / 10000\n",
      "Epoch 25: 1967 / 10000\n",
      "Epoch 26: 2011 / 10000\n",
      "Epoch 27: 2073 / 10000\n",
      "Epoch 28: 2109 / 10000\n",
      "Epoch 29: 2137 / 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"7. Repeat training step 4,5 but for a learning rate of 0.001 and another time for 100.0\")\n",
    "no_hidden_neuron = 30\n",
    "net_eta_001 = network.Network([784, no_hidden_neuron, 10])\n",
    "net_eta_001.SGD(epochs=30, mini_batch_size=10, training_data=training_data, test_data=test_data, eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 977 / 10000\n",
      "Epoch 1: 1004 / 10000\n",
      "Epoch 2: 1001 / 10000\n",
      "Epoch 3: 998 / 10000\n",
      "Epoch 4: 994 / 10000\n",
      "Epoch 5: 989 / 10000\n",
      "Epoch 6: 980 / 10000\n",
      "Epoch 7: 1217 / 10000\n",
      "Epoch 8: 1200 / 10000\n",
      "Epoch 9: 1144 / 10000\n",
      "Epoch 10: 1425 / 10000\n",
      "Epoch 11: 1426 / 10000\n",
      "Epoch 12: 1432 / 10000\n",
      "Epoch 13: 1426 / 10000\n",
      "Epoch 14: 1431 / 10000\n",
      "Epoch 15: 1436 / 10000\n",
      "Epoch 16: 1438 / 10000\n",
      "Epoch 17: 1444 / 10000\n",
      "Epoch 18: 1453 / 10000\n",
      "Epoch 19: 1457 / 10000\n",
      "Epoch 20: 1455 / 10000\n",
      "Epoch 21: 1462 / 10000\n",
      "Epoch 22: 1465 / 10000\n",
      "Epoch 23: 1471 / 10000\n",
      "Epoch 24: 1472 / 10000\n",
      "Epoch 25: 1478 / 10000\n",
      "Epoch 26: 1484 / 10000\n",
      "Epoch 27: 1490 / 10000\n",
      "Epoch 28: 1500 / 10000\n",
      "Epoch 29: 1506 / 10000\n"
     ]
    }
   ],
   "source": [
    "no_hidden_neuron = 30\n",
    "net_eta_100 = network.Network([784, no_hidden_neuron, 10])\n",
    "net_eta_100.SGD(epochs=30, mini_batch_size=10, training_data=training_data, test_data=test_data, eta=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Use the network2.py and repeat 4,5 using cross-entropy cost function,\n",
      "learning rate of 0.5, using 10000 training data samples:\n",
      "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
      "net.large_weight_initializer()\n",
      "net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data,\n",
      "... monitor_evaluation_accuracy=True, monitor_training_cost=True\n",
      "\n",
      "Epoch 0 training complete\n",
      "Cost on training data: 0.881936939233\n",
      "Accuracy on evaluation data: 8416 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.685544725932\n",
      "Accuracy on evaluation data: 8724 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [8416, 8724], [0.88193693923328342, 0.68554472593206606], [])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"\"\"\n",
    "8. Use the network2.py and repeat 4,5 using cross-entropy cost function,\n",
    "learning rate of 0.5, using 10000 training data samples:\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data,\n",
    "... monitor_evaluation_accuracy=True, monitor_training_cost=True\n",
    "\"\"\"\n",
    "print(message)\n",
    "\n",
    "import network2\n",
    "\n",
    "net2 = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net2.large_weight_initializer()\n",
    "evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net2.SGD(\n",
    "    training_data[:10000], \n",
    "    2, \n",
    "    10, \n",
    "    0.5, \n",
    "    evaluation_data=test_data,\n",
    "    monitor_evaluation_accuracy=True,\n",
    "    monitor_training_cost=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
